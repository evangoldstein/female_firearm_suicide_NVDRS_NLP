{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ae55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Section 1. You can use this code to apply your best GB (or other algorithm) model using the\n",
    "##### hyperparameters your found earlier. The following NLP preprocessing steps will be the same\n",
    "##### as explained earlier. The ML modeling steps will be different in Section 2, below.\n",
    "\n",
    "col_names = [\"PersonID\", \"AcuteChronicPain\", \"RecentDispute\", \"RomanticRelationshipProblem\", \"ImmediateFamily_AtScene\", \"IntimatePartner_AtScene\", \"narrative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb447379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(r\"C:\\Users\\Box\\code_files\\narratives_file.csv\", header=0, dialect='excel', encoding = \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64abbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69736ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a5d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[\"narrative\"][25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bace14",
   "metadata": {},
   "outputs": [],
   "source": [
    "###USING BEAUTIFUL SOUP FOR PREPROCESSING\n",
    "\n",
    "from bs4 import BeautifulSoup             \n",
    "\n",
    "example1 = BeautifulSoup(train[\"narrative\"][25]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e5574",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[\"narrative\"][25])\n",
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b618264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Use regular expressions to do a find-and-replace\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      example1.get_text() )  # The text to search\n",
    "print(letters_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3612c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "######NLP Preprocessing: Including token stemming (and lemmatization). Choose either suboption 4b below for Snowball stemming\n",
    "###### or 4a below for lemmatization using #s to disable one or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d76372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "lemmer=WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def narrative_to_words( raw_narrative ):\n",
    "    # Function to convert a raw narrative to a string of words\n",
    "    # The input is a single string (a raw text narrative), and \n",
    "    # the output is a single string (a preprocessed text narrative)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    narrative_text = BeautifulSoup(raw_narrative).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", narrative_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()  \n",
    "    \n",
    "    # 4a. Lemmatize the individual words\n",
    "    #####newcorpus=[' '.join([lemmer.lemmatize(words) for words in text.split(' ')]) for text in words]\n",
    "    \n",
    "    # 4b. Stemming the individual words\n",
    "    newcorpus=[' '.join([stemmer.stem(words) for words in text.split(' ')]) for text in words]\n",
    "    \n",
    "    # 5. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 6. Remove stop words\n",
    "    meaningful_words = [w for w in newcorpus if not w in stops]   \n",
    "    #\n",
    "    # 7. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf35506",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_narrative = narrative_to_words(train[\"narrative\"][25])\n",
    "print(clean_narrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaad217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of text narratives based on the dataframe column size\n",
    "num_narratives = train[\"narrative\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_narratives = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each text narrative; create an index i that goes from 0 to the length\n",
    "# of the text narrative list \n",
    "for i in range( 0, num_narratives ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_narratives.append( narrative_to_words( train[\"narrative\"][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d1b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             ngram_range = (1,2), \\\n",
    "                             stop_words = None,    \\\n",
    "                             max_features = 1000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_narratives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f0c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Convert word tokens frequency to TF-IDF features !\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(train_data_features)\n",
    "\n",
    "\n",
    "# Convert the TF-IDF results to an array\n",
    "train_data_features = X_train_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "###### Section 2. Using the best GB model to predict the binary label of\n",
    "###### interest to unlabeled samples. You can also do this process with a different\n",
    "###### classifier (e.g., RF or SVM).\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e4e60d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Train the GB model using the best hyperparameters identified earlier.\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "### The following hyperparameter values are just examples. Input your own hyperparameters here:\n",
    "gb = GradientBoostingClassifier(criterion='friedman_mse', learning_rate=0.7816184107748736, loss= 'exponential', max_depth=4, max_features='sqrt', min_samples_leaf=2, min_samples_split=10, n_estimators=410, subsample=0.7770667453608728)\n",
    "\n",
    "### Now fit:\n",
    "gb = gb.fit(train_data_features, train[\"IntimatePartner_AtScene\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f25df",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Read a .csv file with unlabeled incident narratives. E.g., a .csv file with two columns:\n",
    "### one with the PersonID and the second with the incident narrative text.\n",
    "test = pd.read_csv(r\"C:\\Users\\Box\\code_files\\unlabeled_narratives_file.csv\", header=0, dialect='excel', encoding = \"utf-8-sig\")\n",
    "\n",
    "print(test.shape)\n",
    "\n",
    "# Create an empty list and append the clean narratives one by one\n",
    "num_narratives = len(test[\"narrative\"])\n",
    "clean_test_narratives = [] \n",
    "\n",
    "for i in range(0,num_narratives):\n",
    "    if( (i+1) % 1000 == 0 ): print(\"Review %d of %d\\n\" % (i+1, num_narratives))\n",
    "    clean_narrative = narrative_to_words(test[\"narrative\"][i] )\n",
    "    clean_test_narratives.append(clean_narrative)\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_narratives)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(test_data_features)\n",
    "\n",
    "\n",
    "# Convert the TF-IDF results to an array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_narrative = narrative_to_words(test[\"narrative\"][1])\n",
    "print(clean_narrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Use the GB model to make outcome label predictions using the unlabeled incident narrative features.\n",
    "result = gb.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35397e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Apply and output the GB model label predictions to a .csv file.\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"label\" column\n",
    "output = pd.DataFrame(data={\"PersonID\":test[\"PersonID\"], \"PREDICTION\":result})\n",
    "\n",
    "# Use pandas to write a CSV output file: This will containt the binary measure (e.g., 0 = no or 1 = yes)\n",
    "# indicating whether it was predicted and labeled that a female firearm suicide decedent experienced\n",
    "# the circumstance of interest (i.e,. through the NLP/ML pipeline). You can replicate this process for\n",
    "# different binary circumstance labels of interest -- just make sure you change the label of interest\n",
    "# throughout the code above.\n",
    "\n",
    "output.to_csv(\"C:\\\\Users\\\\Box\\\\label_predictions_model_GB_labelname.csv\", index=False, quoting=3 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
