{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ae55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Set the column names appearing in the incident narratives .csv file, beginning with the individual case identifier.\n",
    "##### As an example, shown here, we include 7 columns, one each for the case identifier\n",
    "##### (i.e., PersonID) and the incident narrative text, as well as one column for each \n",
    "##### of our five binary labels that we identified and coded through our manual review process.\n",
    "\n",
    "col_names = [\"PersonID\", \"AcuteChronicPain\", \"RecentDispute\", \"RomanticRelationshipProblem\", \"ImmediateFamily_AtScene\", \"IntimatePartner_AtScene\", \"narrative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb447379",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Load your incident narrative data by inputting the file path within double quotation marks below, replacing the sample file path.\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_csv(r\"C:\\Users\\Box\\code_files\\narratives_file.csv\", header=0, dialect='excel', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64abbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###This will tell you the shape of your incident narratives data file.\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69736ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###This will indicate the column names.\n",
    "\n",
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a5d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "###This is a nice check to read a narrative (here the 5th narrative) and ensure the incident narrative text mirrors what is in the .csv file.\n",
    "\n",
    "print(train[\"narrative\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bace14",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Likewise, you can import and use BeautifulSoup to check and ensure the incident narrative text mirrors what is in the .csv file.\n",
    "\n",
    "from bs4 import BeautifulSoup             \n",
    "\n",
    "example1 = BeautifulSoup(train[\"narrative\"][5]) \n",
    "\n",
    "print(train[\"narrative\"][5])\n",
    "print(example1.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### NLP Preprocessing the Incident Narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b618264",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Here we can remove punctuation and non-alphanumeric symbols so that we are left with words.\n",
    "\n",
    "import re\n",
    "# Use regular expressions to do a find-and-replace\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      example1.get_text() )  # The text to search\n",
    "print(letters_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a75a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Download the following NLTK packages.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3612c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Import the stop word list.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d76372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### NLP Preprocessing: Including token stemming (and lemmatization). Choose either suboption 4b below for Snowball stemming\n",
    "###### or 4a below for lemmatization using #s to disable one or the other.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "lemmer=WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def narrative_to_words( raw_narrative ):\n",
    "    # Function to convert a raw narrative to a string of words\n",
    "    # The input is a single string (a raw text narrative), and \n",
    "    # the output is a single string (a preprocessed text narrative)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    narrative_text = BeautifulSoup(raw_narrative).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", narrative_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()  \n",
    "    \n",
    "    # 4a. Lemmatize the individual words\n",
    "    #####newcorpus=[' '.join([lemmer.lemmatize(words) for words in text.split(' ')]) for text in words]\n",
    "    \n",
    "    # 4b. Stemming the individual words\n",
    "    newcorpus=[' '.join([stemmer.stem(words) for words in text.split(' ')]) for text in words]\n",
    "    \n",
    "    # 5. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 6. Remove stop words\n",
    "    meaningful_words = [w for w in newcorpus if not w in stops]   \n",
    "    #\n",
    "    # 7. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf35506",
   "metadata": {},
   "outputs": [],
   "source": [
    "###From our previous example, we can see what the preprocessing steps do to each narrative report.\n",
    "\n",
    "clean_narrative = narrative_to_words(train[\"narrative\"][5])\n",
    "print(clean_narrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaad217",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Get the number of text narratives based on the dataframe column size\n",
    "num_narratives = train[\"narrative\"].size\n",
    "\n",
    "###Initialize an empty list to hold the clean reviews\n",
    "clean_train_narratives = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Loop over each text narrative; create an index i that goes from 0 to the length\n",
    "### of the text narrative list \n",
    "for i in range( 0, num_narratives ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_narratives.append( narrative_to_words( train[\"narrative\"][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d1b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             ngram_range = (1,2), \\\n",
    "                             max_features = 1000) \n",
    "\n",
    "### fit_transform() does two functions: First, it fits the model\n",
    "### and learns the vocabulary; second, it transforms our training data\n",
    "### into feature vectors. The input to fit_transform should be a list of \n",
    "### strings.\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_narratives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f0c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Weight word tokens using TF-IDF technique.\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(train_data_features)\n",
    "\n",
    "### Convert the TF-IDF results to an array\n",
    "\n",
    "tfidf_features = X_train_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Set your features (X) to be the TF-IDF weighted features from the previous step.\n",
    "\n",
    "### IMPORTANT: Set you label of interest (y) here. This will change for different labels.\n",
    "### For example, IntimatePartner_AtScene would be set as our label below.\n",
    "\n",
    "X = tfidf_features\n",
    "y = train[\"IntimatePartner_AtScene\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Take a look at the words in the vocabulary.\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "### Sum the counts of each vocabulary word.\n",
    "\n",
    "dist = np.sum(tfidf_features, axis=0)\n",
    "\n",
    "### For each, print the vocabulary word and the frequency it \n",
    "### appears in the training set.\n",
    "\n",
    "for tag, count in zip(vocab, dist): print(count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "###### Supervised Machine Learning: Classifying and Testing Mode Performance\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#####Gradient Boosting (GB) Classifier\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5cd07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Create a 20% hold-out test set\n",
    "### If you are using this process to tune and identify a best model's hyperparameters,\n",
    "### it is a good practice to conduct this experiment with multiple (e.g., 5) random train_test_split\n",
    "### seeds (e.g, random_state = 72, below) and multiple random model seeds (e.g., random_state=891, below).\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.20, random_state=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab58fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Defining the GB hyperparameter choices. You can learn more about this classifier\n",
    "### and the hyperparameter options at \n",
    "### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html.\n",
    "\n",
    "from time import time\n",
    "import scipy.stats as stats\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from scipy.stats import uniform, loguniform\n",
    "from scipy.stats import uniform as sp_randFloat\n",
    "from scipy.stats import randint as sp_randInt  \n",
    "\n",
    "param_grid_gb = params = {\n",
    "                  \"loss\": ['exponential'],\n",
    "                  \"learning_rate\": sp_randFloat(),\n",
    "                  \"subsample\"    : sp_randFloat(),\n",
    "                  \"n_estimators\" : sp_randInt(100, 1000),\n",
    "                  \"max_depth\"    : sp_randInt(4, 10),\n",
    "                  \"min_samples_split\" : [2, 5, 10],\n",
    "                  \"min_samples_leaf\": [1, 2, 4],\n",
    "                  \"max_features\" : [\"log2\",\"sqrt\"],\n",
    "                  \"criterion\": [\"friedman_mse\",  \"mse\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe86cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Conduct 5-fold CV to validate the model while optimizing the model hyperparameters to maximize F1.\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=891)\n",
    "\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 50 different combinations, and use all available cores\n",
    "clf_random_gb = RandomizedSearchCV(estimator=gb, param_distributions=param_grid_gb,\n",
    "                              scoring='f1', n_iter = 100,\n",
    "                              cv = 5, verbose=2, n_jobs=-1, random_state=891)\n",
    "\n",
    "# Fit the random search model\n",
    "clf_random_gb.fit(X_train, Y_train);\n",
    "Y_pred=clf_random_gb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0f7554",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Dispay the best model hyperparameters identified through the process above. You will use these later to\n",
    "### specify your best GB model when applying the label of interest to your broader sample of unlabeled \n",
    "### data in NVDRS.\n",
    "\n",
    "clf_random_gb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96717ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, Y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    errors = abs(predictions - Y_test)\n",
    "    mape = 100 * np.mean(errors / Y_test)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f}'.format(np.mean(errors)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ca07b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Examine the best model's performance metrics for this combination of random seeds.\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test,Y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac2ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###You can also examine specific performance metrics as such.\n",
    "\n",
    "roc_auc_score(Y_test, Y_pred)\n",
    "#roc_auc_score(Y_test, clf_random_gb.best_estimator_.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
